{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import time\n",
    "import ssl\n",
    "import re\n",
    "import csv\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.trulia.com'\n",
    "url = 'https://www.trulia.com/for_sale/Columbia,MO/1p_beds/SINGLE-FAMILY_HOME_type/'\n",
    "headers = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'en-US,en;q=0.9,es;q=0.8',\n",
    "    #'cache-control': 'max-age=0',\n",
    "    'dnt': '1',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'cross-site',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'sec-gpc': '1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.101 Safari/537.36'\n",
    "    }\n",
    "\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "r = requests.get(url, headers=headers, verify=False)\n",
    "soup = bs(r.text, 'html.parser')"
   ]
  },
  {
   "source": [
    "## Single Search Page"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    \"\"\"Returns a beautiful soup object.\"\"\"\n",
    "    r = requests.get(url, headers=headers, verify=False)\n",
    "    \n",
    "    if not r.ok:\n",
    "        print('Server Responded: ', r.status_code)\n",
    "    else:\n",
    "        soup = bs(r.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "\n",
    "def get_main_attrs(soup):\n",
    "    \"\"\"Gets the attributes of each listing from a search page and returns\n",
    "    a dictionary where the address is the key and the attributes are the values.\"\"\"\n",
    "    addr_ = soup.find_all('div',attrs={'data-testid': 'property-street'})\n",
    "    addr = [x.get_text().strip() for x in addr_]\n",
    "    region_ = soup.find_all('div',attrs={'data-testid': 'property-region'})\n",
    "    region = [x.get_text().strip() for x in region_]\n",
    "    new_ = soup.find_all('div', attrs={'data-testid': 'property-tags'})\n",
    "    new = [x.get_text().strip() if x.get_text() is not '' else 'OLD' for x in new_]\n",
    "    prices_ = soup.find_all('div', attrs={'data-testid': 'property-price'})\n",
    "    prices = [x.get_text().strip() for x in prices_]\n",
    "    bedrooms_ = soup.find_all('div', attrs={'data-testid': 'property-beds'})\n",
    "    bedrooms = [x.get_text().strip() for x in bedrooms_]\n",
    "    baths_ = soup.find_all('div', attrs={'data-testid': 'property-baths'})\n",
    "    baths = [x.get_text().strip() for x in baths_]\n",
    "    sqft_ = soup.find_all('div', attrs={'data-testid': 'property-floorSpace'})\n",
    "    sqft = [x.get_text().strip() for x in sqft_]\n",
    "\n",
    "    c = zip(addr, region, new, prices, bedrooms, baths, sqft)\n",
    "    d = {}\n",
    "\n",
    "    for a, *x in c:\n",
    "        d.setdefault(a, []).append(x)\n",
    "\n",
    "    for k, v in d.items():\n",
    "        d[k] = v[0]\n",
    "\n",
    "    df = pd.DataFrame(d).T.rename(columns={0:'region', 1:'new', 2:'price', 3:'bedrm', 4:'bth', 5:'sqft'})\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_urls(soup):\n",
    "    \"\"\"Gets the external URLs on the page.\"\"\"\n",
    "    hrefs = []\n",
    "\n",
    "    for listing in soup.find_all('div', attrs={'data-testid': 'home-card-sale'}):\n",
    "        for link in listing.find_all('a', attrs={'href': re.compile('^/')}):\n",
    "            hrefs.append(link)\n",
    "        \n",
    "    return [base_url + x['href'] for x in hrefs]"
   ]
  },
  {
   "source": [
    "## Single Listing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_url = 'https://www.trulia.com/p/mo/columbia/2815-wild-plum-ct-columbia-mo-65201--2060813753'\n",
    "\n",
    "page = get_page(s_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_attrs(soup):\n",
    "    \"\"\"Gets a single page's attributes and returns a dataframe.\"\"\"\n",
    "    # Getting address again to map it with the address collected on main page\n",
    "    try:\n",
    "        addr_p = soup.find('span', attrs={'data-testid': 'home-details-summary-headline'}).get_text()\n",
    "    except:\n",
    "        addr_p = ''\n",
    "\n",
    "    # Crime\n",
    "    try:\n",
    "        crime = soup.find('div',attrs={'aria-label': 'Crime'}).get_text()\n",
    "    except:\n",
    "        crime = ''\n",
    "    # Schools in the area\n",
    "    try:\n",
    "        schools = soup.find('div', attrs={'aria-label': 'Schools'}).get_text()\n",
    "    except:\n",
    "        schools = ''\n",
    "\n",
    "    # Home Details -- (heating, roof, etc.)\n",
    "    try:\n",
    "        details_ = soup.find('ul', attrs={'data-testid': 'home-features'})\n",
    "        details = [x.get_text() for x in details_.find_all('li')]\n",
    "    except:\n",
    "        details = ''\n",
    "\n",
    "    # History of listings\n",
    "    try:\n",
    "        list_hist_ = soup.find('div', attrs={'data-testid': 'price-history-container'}).find('table').find_all('tr')\n",
    "        list_hist = [x.get_text().strip() for x in list_hist_]\n",
    "    except:\n",
    "        list_hist = ''\n",
    "\n",
    "    # Taxes on the house\n",
    "    try:\n",
    "        tax_table_ = soup.find('div', attrs={'data-testid': 'property-tax-container'}).find('table').find_all('tr')\n",
    "        tax_table = [x.get_text().strip() for x in tax_table_]\n",
    "    except:\n",
    "        tax_table = ''\n",
    "\n",
    "    try:\n",
    "        price_trends_ = soup.find_all('div', attrs={'class': 'Text__TextBase-sc-1i9uasc-0-div Text__TextContainerBase-sc-1i9uasc-1 epkfvN'})\n",
    "        # Typical home value compared to others like it\n",
    "        typ_home_val = [x.get_text() for x in price_trends_][-2] \n",
    "        # Typical price per sqft of houses similar to it\n",
    "        typ_sqft_pri = [x.get_text() for x in price_trends_][-1]\n",
    "    except:\n",
    "        typ_home_val = ''\n",
    "        typ_sqft_pri = ''\n",
    "\n",
    "    try:\n",
    "        pcts_ = soup.find_all('span', attrs={'class': 'Text__TextBase-sc-1i9uasc-0 fUDZSu'})\n",
    "        # How much this house varies from others (based on typ_home_val)\n",
    "        val_pct = [x.get_text() for x in pcts_][-2]\n",
    "        # How much this house varies from others (based on typ_sqft_pri)\n",
    "        sqft_pct = [x.get_text() for x in pcts_][-1]\n",
    "    except:\n",
    "        val_pct = ''\n",
    "        sqft_pct = ''\n",
    "\n",
    "    l = [addr_p, crime, schools, details, list_hist, tax_table, typ_home_val, val_pct, typ_sqft_pri, sqft_pct]\n",
    "\n",
    "    names = ['addr', 'crime', 'schools', 'details', 'list_hist', 'tax', 'typ_val', 'val_pct', 'typ_sqft', 'sqft_pct']\n",
    "\n",
    "    df = pd.DataFrame(l).T\n",
    "\n",
    "    return df.rename(columns=dict(zip(df.columns.values, names)))"
   ]
  },
  {
   "source": [
    "## Using a Single Search page to Scrape 30 Listings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spage = 'https://www.trulia.com/for_sale/Columbia,MO/1p_beds/SINGLE-FAMILY_HOME_type/2_p/'\n",
    "\n",
    "l_urls = get_urls(get_page(spage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Listing 0---- completed\n",
      "Listing 1---- completed\n",
      "Listing 2---- completed\n",
      "Listing 3---- completed\n",
      "Listing 4---- completed\n",
      "Listing 5---- completed\n",
      "Listing 6---- completed\n",
      "Listing 7---- completed\n",
      "Listing 8---- completed\n",
      "Listing 9---- completed\n",
      "Listing 10--- completed\n",
      "Listing 11--- completed\n",
      "Listing 12--- completed\n",
      "Listing 13--- completed\n",
      "Listing 14--- completed\n",
      "Listing 15--- completed\n",
      "Listing 16--- completed\n",
      "Listing 17--- completed\n",
      "Listing 18--- completed\n",
      "Listing 19--- completed\n",
      "Listing 20--- completed\n",
      "Listing 21--- completed\n",
      "Listing 22--- completed\n",
      "Listing 23--- completed\n",
      "Listing 24--- completed\n",
      "Listing 25--- completed\n",
      "Listing 26--- completed\n",
      "Listing 27--- completed\n",
      "Listing 28--- completed\n",
      "Listing 29--- completed\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "\n",
    "for i, url in enumerate(l_urls):\n",
    "    df = get_page_attrs(get_page(url))\n",
    "    dfs.append(df)\n",
    "    print(f'Listing {i:-<5} completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = get_main_attrs(get_page(spage)).rename_axis('addr').dropna(how='all')\n",
    "df_ind = pd.concat(dfs).set_index('addr').dropna(how='all')\n",
    "\n",
    "# df_main.join(df_ind, how='outer').dropna(how='all')\n",
    "pd.merge(df_main, df_ind, left_index=True, right_index=True).head(1)"
   ]
  },
  {
   "source": [
    "## Getting Main Attributes for Multiple Pages\n",
    "- I got the ranges to use on the website by viewing the number of pages available"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "como = [f'https://www.trulia.com/for_sale/Columbia,MO/1p_beds/SINGLE-FAMILY_HOME_type/{x}_p/' for x in range(1, 8)]\n",
    "kc = [f'https://www.trulia.com/for_sale/Kansas_City,MO/1p_beds/SINGLE-FAMILY_HOME_type/{x}_p/' for x in range(1, 29)]\n",
    "stl = [f'https://www.trulia.com/for_sale/Saint_Louis,MO/1p_beds/SINGLE-FAMILY_HOME_type/{x}_p/' for x in range(1, 38)]\n",
    "spring = [f'https://www.trulia.com/for_sale/Springfield,MO/1p_beds/SINGLE-FAMILY_HOME_type/{x}_p/' for x in range(1, 15)]\n",
    "lees = [f'https://www.trulia.com/for_sale/Lees_Summit,MO/1p_beds/SINGLE-FAMILY_HOME_type/{x}_p/' for x in range(1, 16)]\n",
    "\n",
    "list_all_urls = [como, kc, stl, spring, lees]\n",
    "all_urls = list(chain.from_iterable(list_all_urls))\n",
    "\n",
    "len(all_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "URL 5---- completed\n",
      "URL 10--- completed\n",
      "URL 15--- completed\n",
      "URL 20--- completed\n",
      "URL 25--- completed\n",
      "URL 30--- completed\n",
      "URL 35--- completed\n",
      "URL 40--- completed\n",
      "URL 45--- completed\n",
      "URL 50--- completed\n",
      "URL 55--- completed\n",
      "URL 60--- completed\n",
      "URL 65--- completed\n",
      "URL 70--- completed\n",
      "URL 75--- completed\n",
      "URL 80--- completed\n",
      "URL 85--- completed\n",
      "URL 90--- completed\n",
      "URL 95--- completed\n",
      "URL 100-- completed\n"
     ]
    }
   ],
   "source": [
    "all_main = []\n",
    "\n",
    "for i, url in enumerate(all_urls):\n",
    "    df = get_main_attrs(get_page(url))\n",
    "    all_main.append(df)\n",
    "    if (i+1) % 5 == 0:\n",
    "        print(f'URL {i+1:-<5} completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               region  \\\n",
       "SomerBrook                            Kansas City, MO   \n",
       "5620 W Ruben Ln                       Springfield, MO   \n",
       "3155 S Bobcat Ct                         Columbia, MO   \n",
       "3533 Paris Ave     The Greater Ville, Saint Louis, MO   \n",
       "2815 Wild Plum Ct                        Columbia, MO   \n",
       "\n",
       "                                              new      price bedrm  bth  \\\n",
       "SomerBrook         NEW CONSTRUCTIONBUILDABLE PLAN  $380,700+   4bd  3ba   \n",
       "5620 W Ruben Ln                 FOR SALE BY OWNER   $699,900   5bd  4ba   \n",
       "3155 S Bobcat Ct                              OLD   $599,900   5bd  3ba   \n",
       "3533 Paris Ave                  FOR SALE BY OWNER    $10,500   3bd  1ba   \n",
       "2815 Wild Plum Ct                             OLD   $550,000   5bd  5ba   \n",
       "\n",
       "                         sqft  \n",
       "SomerBrook         2,137 sqft  \n",
       "5620 W Ruben Ln    4,830 sqft  \n",
       "3155 S Bobcat Ct   4,690 sqft  \n",
       "3533 Paris Ave     1,426 sqft  \n",
       "2815 Wild Plum Ct  4,810 sqft  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>region</th>\n      <th>new</th>\n      <th>price</th>\n      <th>bedrm</th>\n      <th>bth</th>\n      <th>sqft</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>SomerBrook</th>\n      <td>Kansas City, MO</td>\n      <td>NEW CONSTRUCTIONBUILDABLE PLAN</td>\n      <td>$380,700+</td>\n      <td>4bd</td>\n      <td>3ba</td>\n      <td>2,137 sqft</td>\n    </tr>\n    <tr>\n      <th>5620 W Ruben Ln</th>\n      <td>Springfield, MO</td>\n      <td>FOR SALE BY OWNER</td>\n      <td>$699,900</td>\n      <td>5bd</td>\n      <td>4ba</td>\n      <td>4,830 sqft</td>\n    </tr>\n    <tr>\n      <th>3155 S Bobcat Ct</th>\n      <td>Columbia, MO</td>\n      <td>OLD</td>\n      <td>$599,900</td>\n      <td>5bd</td>\n      <td>3ba</td>\n      <td>4,690 sqft</td>\n    </tr>\n    <tr>\n      <th>3533 Paris Ave</th>\n      <td>The Greater Ville, Saint Louis, MO</td>\n      <td>FOR SALE BY OWNER</td>\n      <td>$10,500</td>\n      <td>3bd</td>\n      <td>1ba</td>\n      <td>1,426 sqft</td>\n    </tr>\n    <tr>\n      <th>2815 Wild Plum Ct</th>\n      <td>Columbia, MO</td>\n      <td>OLD</td>\n      <td>$550,000</td>\n      <td>5bd</td>\n      <td>5ba</td>\n      <td>4,810 sqft</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 144
    }
   ],
   "source": [
    "df_main = pd.concat(all_main)\n",
    "df_main.sample(5)\n",
    "# df_main.to_csv('df_main.csv', index=True, columns=df_main.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = pd.read_csv('df/df_main.csv', index_col=0)"
   ]
  },
  {
   "source": [
    "## Getting all of the individual listing's URLs\n",
    "- I had to separate the two for loops (main page attributes/individual page attributes) because I kept getting blocked/captcha."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "URL 20--- completed trulia.com/p/mo/kansas-city/\n",
      "URL 40--- completed trulia.com/p/mo/saint-louis/\n",
      "URL 60--- completed trulia.com/p/mo/saint-louis/\n",
      "URL 80--- completed trulia.com/p/mo/springfield/\n",
      "URL 100-- completed trulia.com/property/73163164\n"
     ]
    }
   ],
   "source": [
    "ind_urls = []\n",
    "\n",
    "for idx, url in enumerate(all_urls):\n",
    "    urls = get_urls(get_page(url))\n",
    "    ind_urls.append(urls)\n",
    "    time.sleep(3)\n",
    "    if (idx+1) % 20 == 0:\n",
    "        print(f'URL {idx+1:-<5} completed {urls[0][12:40]:-<5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3108"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "page_urls = list(chain.from_iterable(ind_urls))\n",
    "\n",
    "with open('pickle/page_urls.pickle', 'wb') as f:\n",
    "    pickle.dump(page_urls, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "len(page_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "URL 50---completed---columbia/315\n",
      "URL 100--completed---columbia/240\n",
      "URL 150--completed---columbia/490\n",
      "URL 200--completed---columbia/lot\n",
      "URL 250--completed---kansas-city/\n",
      "URL 300--completed---kansas-city/\n",
      "URL 350--completed---kansas-city/\n",
      "URL 400--completed---kansas-city/\n",
      "URL 450--completed---kansas-city/\n",
      "URL 500--completed---kansas-city/\n",
      "URL 550--completed---kansas-city/\n",
      "URL 600--completed---kansas-city/\n",
      "URL 650--completed---kansas-city/\n",
      "URL 700--completed---er-community\n",
      "URL 750--completed---er-community\n",
      "URL 800--completed---kansas-city/\n",
      "URL 850--completed---kansas-city/\n",
      "URL 900--completed---er-community\n",
      "URL 950--completed---raytown/5532\n",
      "URL 1000-completed---kansas-city/\n",
      "URL 1050-completed---lees-summit/\n",
      "URL 1100-completed---kansas-city/\n",
      "URL 1150-completed---saint-louis/\n",
      "URL 1200-completed---saint-louis/\n",
      "URL 1250-completed---saint-louis/\n",
      "URL 1300-completed---saint-louis/\n",
      "URL 1350-completed---saint-louis/\n",
      "URL 1400-completed---saint-louis/\n",
      "URL 1450-completed---saint-louis/\n",
      "URL 1500-completed---saint-louis/\n",
      "URL 1550-completed---saint-louis/\n",
      "URL 1600-completed---saint-louis/\n",
      "URL 1650-completed---saint-louis/\n",
      "URL 1700-completed---saint-louis/\n",
      "URL 1750-completed---saint-louis/\n",
      "URL 1800-completed---saint-louis/\n",
      "URL 1850-completed---saint-louis/\n",
      "URL 1900-completed---saint-louis/\n",
      "URL 1950-completed---saint-louis/\n",
      "URL 2000-completed---saint-louis/\n",
      "URL 2050-completed---saint-louis/\n",
      "URL 2100-completed---saint-louis/\n",
      "URL 2150-completed---saint-louis/\n",
      "URL 2200-completed---hazelwood/69\n",
      "URL 2250-completed---springfield/\n",
      "URL 2300-completed---springfield/\n",
      "URL 2350-completed---springfield/\n",
      "URL 2400-completed---springfield/\n",
      "URL 2450-completed---springfield/\n",
      "URL 2500-completed---springfield/\n",
      "URL 2550-completed---springfield/\n",
      "URL 2600-completed---er-community\n",
      "URL 2650-completed---nixa/201-s-d\n",
      "URL 2700-completed---lees-summit/\n",
      "URL 2750-completed---er-community\n",
      "URL 2800-completed---rty/90772676\n",
      "URL 2850-completed---lees-summit/\n",
      "URL 2900-completed---rty/72421008\n",
      "URL 2950-completed---er-community\n",
      "URL 3000-completed---er-community\n",
      "URL 3050-completed---rty/50747782\n",
      "URL 3100-completed---raymore/1900\n"
     ]
    }
   ],
   "source": [
    "ind_dfs = []\n",
    "\n",
    "for idx, url in enumerate(page_urls):\n",
    "    df = get_page_attrs(get_page(url))\n",
    "    ind_dfs.append(df)\n",
    "    time.sleep(2)\n",
    "\n",
    "    if (idx+1) % 50 == 0:\n",
    "        print(f'URL {idx+1:-<5}completed{url[28:40]:->15}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the list of dataframes\n",
    "with open('pickle/df_ind.pickle', 'wb') as f:\n",
    "    pickle.dump(ind_dfs, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('pickle/df_ind.pickle', 'rb') as f:\n",
    "    ind_list = pickle.load(f)\n",
    "\n",
    "# Saving the concatenated dataframes\n",
    "df_ind = pd.concat(ind_dfs).replace('', np.nan).dropna(how='all').set_index('addr')\n",
    "df_ind.to_csv('df/df_ind.csv', index=True, columns=df_ind.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "df_main: (2919, 6), df_ind: (3436, 9)\n"
     ]
    }
   ],
   "source": [
    "print(f'df_main: {df_main.shape}, df_ind: {df_ind.shape}')"
   ]
  }
 ]
}