{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11.999999999999998'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =======================================================\n",
    "# NOTE: Piping in Python (similar to %>% in R, | in Unix)\n",
    "from datetime import datetime as dt\n",
    "from itertools import chain\n",
    "import math\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from matplotlib import test\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from sspipe import p, px\n",
    "\n",
    "import tabloo                             # dataframe visualizer\n",
    "# import dfply                            # like dplyr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://slashdot.org/\"\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "    \"accept-encoding\": \"gzip, deflate, br\",\n",
    "    \"accept-language\": \"en-US,en;q=0.9,es;q=0.8\",\n",
    "    # 'cache-control': 'max-age=0',\n",
    "    \"dnt\": \"1\",\n",
    "    \"sec-fetch-dest\": \"document\",\n",
    "    \"sec-fetch-mode\": \"navigate\",\n",
    "    \"sec-fetch-site\": \"cross-site\",\n",
    "    \"sec-fetch-user\": \"?1\",\n",
    "    \"sec-gpc\": \"1\",\n",
    "    \"upgrade-insecure-requests\": \"1\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.101 Safari/537.36\",\n",
    "}\n",
    "\n",
    "r = requests.get(url, headers=headers)\n",
    "soup = bs(r.text, \"html.parser\")\n",
    "\n",
    "#### ┌─────────────────────────────────────────────────┐\n",
    "#### │ Scraping one page                               │\n",
    "#### └─────────────────────────────────────────────────┘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-coaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title, link, rating\n",
    "tit = [t.get_text() for t in soup.select(\"h2 span a\")]\n",
    "\n",
    "# Title of post\n",
    "title = [j for i in soup.find_all(\"span\", class_=\"story-title\") for j in i.find(\"a\")]\n",
    "\n",
    "# Date/time of post\n",
    "import dateutil.parser\n",
    "# dateutil.parser.parse('string')\n",
    "dated = [d.get_text() for d in soup.select(\"span.story-byline time\")]\n",
    "date = [re.sub(\"on|@\", \"\", x).strip() for x in dated]\n",
    "dt = [dt.strptime(d, \"%A %B %d, %Y %I:%M%p\") for d in date]\n",
    "\n",
    "# External link to post\n",
    "elink = [l.text.strip() for l in soup.select(\"h2 span span\")]\n",
    "\n",
    "# Comments on post\n",
    "comments = (\n",
    "    [x.get_text() for x in soup.select(\"span.comment-bubble a\")]\n",
    "    | p(np.array)\n",
    "    | px.astype(\"int\")\n",
    ")\n",
    "\n",
    "# Category of post\n",
    "cat = [b.get(\"alt\") for b in soup.find_all(\"img\")] | p(list, p(filter, None, px))\n",
    "# Using this sort of as a try except in case it were to not exist\n",
    "category = [x.replace(\"Icon\", \"\") for x in cat] | p(filter, None) | p(list)\n",
    "\n",
    "# User who made the post\n",
    "user = [\n",
    "    u.get_text(\" \", strip=True).replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "    for u in soup.select(\"span.story-byline\")\n",
    "]\n",
    "user = [\" \".join(a.split()) | p(re.findall, r\"Postedby\\s(\\w+)\", px) for a in user]\n",
    "\n",
    "# Popularity of post (ratings? red?)\n",
    "pop = [\n",
    "    re.findall(\"'([a-zA-Z0-9,\\s]*)'\", prop[\"onclick\"]) | px[1]\n",
    "    for prop in soup.find_all(\"span\", attrs={\"alt\": \"Popularity\"})\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ┌─────────────────────────────────────────────────┐\n",
    "### │ Dataframe                                       │\n",
    "### └─────────────────────────────────────────────────┘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"title\": title,\n",
    "        \"date\": dt,\n",
    "        \"exlink\": elink,\n",
    "        \"comments\": comments,\n",
    "        \"category\": category,\n",
    "        \"user\": user,\n",
    "        \"popular\": pop,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "tabloo.show(df)\n",
    "# from operator import itemgetter\n",
    "# df['user'] = df['user'] | p(map, p(itemgetter(0)), px) | p(list)\n",
    "\n",
    "df[\"user\"] = df[\"user\"] | p(chain.from_iterable) | p(list)\n",
    "df[\"exlink\"] = df[\"exlink\"] | px.str.replace(r\"\\(|\\)\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-worthy",
   "metadata": {},
   "source": [
    "### ┌─────────────────────────────────────────────────┐\n",
    "### │ Functions for scraping more than one page       │\n",
    "### └─────────────────────────────────────────────────┘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "lines_to_end_of_cell_marker": 0,
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_page(url, sleep=False, prnt=False):\n",
    "    response = requests.get(url)\n",
    "    if not response.ok:\n",
    "        print(\"Server Responded: \", response.status_code)\n",
    "    else:\n",
    "        soup = bs(response.text, \"lxml\")\n",
    "        patt = re.compile(r'\\d+')\n",
    "        if sleep:\n",
    "            time.sleep(3)\n",
    "        if prnt:\n",
    "            print(f'Number {patt.search(url).group(): ^5}done')\n",
    "    return soup\n",
    "\n",
    "\n",
    "def get_data(soup):\n",
    "    pattern = re.compile(r\"\\([a-z0-9.\\-]+[.](\\w+)\\)\")\n",
    "\n",
    "    try:\n",
    "        title_ = [x.get_text(' ', strip=True) for x in soup.select(\"h2 span.story-title\")]\n",
    "        title = [pattern.sub('', x).strip() for x in title_]\n",
    "    except:\n",
    "        title = \"\"\n",
    "\n",
    "    try:\n",
    "        dated = [d.get_text(\" \", strip=True) for d in soup.select(\"span.story-byline time\")]\n",
    "        date = [re.sub(\"on|@\", \"\", x).strip() for x in dated]\n",
    "    except:\n",
    "        date = \"\"\n",
    "\n",
    "    try:\n",
    "        curls = {}\n",
    "        ex = [x.get_text(' ', strip=True) for x in soup.select(\"h2 span.story-title\")]\n",
    "\n",
    "        for idx, u in enumerate(ex):\n",
    "            if not pattern.search(u):\n",
    "                curls[idx] = \"Empty\"\n",
    "            else:\n",
    "                curls[idx] = pattern.search(u).group()\n",
    "\n",
    "        elink = list(curls.values())\n",
    "    except:\n",
    "        try:\n",
    "            elink = [l.text.strip() for l in soup.select(\"h2 span span.no\")]\n",
    "        except:\n",
    "            elink = \"\"\n",
    "\n",
    "    try:\n",
    "        comments = (\n",
    "            [x.get_text() for x in soup.select(\"span.comment-bubble a\")]\n",
    "            | p(np.array)\n",
    "            | px.astype(\"int\"))\n",
    "    except:\n",
    "        comments = \"\"\n",
    "\n",
    "    try:\n",
    "        cat = ([b.get(\"alt\") for b in soup.find_all(\"img\")]\n",
    "              | p(list, p(filter, None, px)))\n",
    "        category = ([x.replace(\"Icon\", \"\") for x in cat]\n",
    "                    | p(filter, None)\n",
    "                    | p(list))\n",
    "    except:\n",
    "        category = \"\"\n",
    "\n",
    "    try:\n",
    "        user = [u.get_text(\" \", strip=True).replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "                for u in soup.select(\"span.story-byline\")]\n",
    "        user = [\" \".join(a.split())\n",
    "                | p(re.findall, r\"Postedby\\s(\\w+)\", px) for a in user]\n",
    "    except:\n",
    "        user = \"\"\n",
    "\n",
    "    try:\n",
    "        pop = [re.findall(\"'([a-zA-Z0-9,\\s]*)'\", prop[\"onclick\"]) | px[1]\n",
    "            for prop in soup.find_all(\"span\", attrs={\"alt\": \"Popularity\"})]\n",
    "    except:\n",
    "        pop = \"\"\n",
    "\n",
    "    temp =pd.DataFrame({\n",
    "            \"title\": title,\n",
    "            \"date\": date,\n",
    "            \"exlink\": elink,\n",
    "            \"comments\": comments,\n",
    "            \"category\": category,\n",
    "            \"user\": user,\n",
    "            \"popular\": pop\n",
    "        }\n",
    "    )\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-enclosure",
   "metadata": {},
   "source": [
    "### ┌─────────────────────────────────────────────────┐\n",
    "### │ Scraping more than one page                     │\n",
    "### └─────────────────────────────────────────────────┘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "base_url = \"https://slashdot.org/?page=\"\n",
    "urls = [base_url + str(i) for i in range(1, 100)]\n",
    "\n",
    "test_u = urls | p(sample, 2)\n",
    "\n",
    "\n",
    "data = [get_data(get_page(x, sleep=True, prnt=True)) for x in urls] | p(pd.concat, px)\n",
    "\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "data.to_csv('df.csv', index=True, columns=data.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-huntington",
   "metadata": {},
   "source": [
    "### ┌─────────────────────────────────────────────────┐\n",
    "### │ Reading back in & cleaning                      │\n",
    "### └─────────────────────────────────────────────────┘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-burns",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('df.csv', index_col=0)\n",
    "\n",
    "df.columns.values\n",
    "df.dtypes\n",
    "\n",
    "# Date\n",
    "df['date'] = [' '.join(x.split(' ')[1:]) | p(dt.strptime, \"%B %d, %Y %I:%M%p\") for x in df['date']]\n",
    "df[\"exlink\"] = df[\"exlink\"] | px.str.replace(r\"\\(|\\)\", \"\", regex=True)\n",
    "df[\"category\"] = df[\"category\"].astype(\"category\")\n",
    "df[\"user\"] = df[\"user\"] | px.str.replace(r\"\\[|\\]|\\'\", \"\", regex=True)\n",
    "df[\"popular\"] = df[\"popular\"].astype(\"category\")\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-button",
   "metadata": {},
   "source": [
    "### ┌─────────────────────────────────────────────────┐\n",
    "### │ Scraping post pages for comments                │\n",
    "### └─────────────────────────────────────────────────┘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main page to post page\n",
    "test = urls[0]\n",
    "soup = get_page(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-iraqi",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Internal links to other posts\n",
    "ilink_ = [x['href'] for x in soup.select('h2 span a')] | p(filter, None) | p(list)\n",
    "ilink = [re.sub(\"//\", \"\", x) for x in ilink_[::2] if x.startswith(\"//\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-gregory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post page\n",
    "ppage = \"https://news.slashdot.org/story/21/02/11/0029259/wall-street-fund-wants-to-hire-rwallstreetbets-users-to-help-pick-meme-stocks#comments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-forestry",
   "metadata": {},
   "outputs": [],
   "source": [
    "psoup = get_page(ppage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title / External Link / Comments\n",
    "tec = [x.get_text() for x in psoup.select(\"h2 span a\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptitle, pelink = [x.get_text(' ', strip=True) for x in psoup.select(\"h2 span.story-title a\")]\n",
    "pcomm = [x.get_text() for x in psoup.select((\"span.comment-bubble\"))] | px[0]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Cat",
   "language": "python",
   "name": "cat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
